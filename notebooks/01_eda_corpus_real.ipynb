{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA — Real BAPRO Corpus\n",
    "\n",
    "Exploratory data analysis of the ingested article corpus and FSI target.\n",
    "Run after `make backfill` and `make seed_fsi` have populated the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Allow imports from repo root\n",
    "repo_root = Path().resolve().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "from db.connection import get_engine\n",
    "\n",
    "engine = get_engine()\n",
    "\n",
    "# Load articles\n",
    "with engine.connect() as conn:\n",
    "    articles_df = pd.read_sql(\n",
    "        text(\"SELECT id, date, headline, gdelt_themes, source FROM articles ORDER BY date\"),\n",
    "        conn,\n",
    "    )\n",
    "    fsi_df = pd.read_sql(\n",
    "        text(\"SELECT date, fsi_value FROM fsi_target ORDER BY date\"),\n",
    "        conn,\n",
    "    )\n",
    "    emb_count = conn.execute(text(\"SELECT COUNT(*) FROM article_embeddings\")).scalar()\n",
    "\n",
    "articles_df[\"date\"] = pd.to_datetime(articles_df[\"date\"])\n",
    "fsi_df[\"date\"] = pd.to_datetime(fsi_df[\"date\"])\n",
    "\n",
    "print(f\"Articles: {len(articles_df):,}\")\n",
    "print(f\"Embeddings: {emb_count:,}\")\n",
    "print(f\"FSI rows: {len(fsi_df):,}\")\n",
    "print(f\"Date range: {articles_df['date'].min().date()} to {articles_df['date'].max().date()}\")\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Temporal heatmap: articles per week x day-of-week\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # non-interactive backend for headless execution\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "docs_dir = Path(\"../docs\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "articles_df[\"week\"] = articles_df[\"date\"].dt.isocalendar().week.astype(int)\n",
    "articles_df[\"year\"] = articles_df[\"date\"].dt.year\n",
    "articles_df[\"year_week\"] = articles_df[\"year\"].astype(str) + \"-W\" + articles_df[\"week\"].astype(str).str.zfill(2)\n",
    "articles_df[\"dow\"] = articles_df[\"date\"].dt.dayofweek  # 0=Mon\n",
    "\n",
    "pivot = articles_df.groupby([\"year_week\", \"dow\"]).size().unstack(fill_value=0)\n",
    "pivot.columns = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"][: len(pivot.columns)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, max(4, len(pivot) * 0.4)))\n",
    "im = ax.imshow(pivot.values.T, aspect=\"auto\", cmap=\"YlOrRd\")\n",
    "ax.set_xticks(range(len(pivot)))\n",
    "ax.set_xticklabels(pivot.index, rotation=90, fontsize=7)\n",
    "ax.set_yticks(range(len(pivot.columns)))\n",
    "ax.set_yticklabels(pivot.columns)\n",
    "ax.set_title(\"Articles per week x day-of-week\")\n",
    "plt.colorbar(im, ax=ax, label=\"article count\")\n",
    "plt.tight_layout()\n",
    "out_path = docs_dir / \"eda_temporal_heatmap.png\"\n",
    "fig.savefig(out_path, dpi=100)\n",
    "print(f\"Saved heatmap to {out_path}\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Token distribution: headline word count histogram\n",
    "articles_df[\"word_count\"] = articles_df[\"headline\"].fillna(\"\").str.split().str.len()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(articles_df[\"word_count\"], bins=40, color=\"steelblue\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Headline word count\")\n",
    "ax.set_ylabel(\"Number of articles\")\n",
    "ax.set_title(\"Headline token distribution\")\n",
    "ax.axvline(articles_df[\"word_count\"].median(), color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Median: {articles_df['word_count'].median():.0f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "out_path2 = docs_dir / \"eda_token_distribution.png\"\n",
    "fig.savefig(out_path2, dpi=100)\n",
    "print(f\"Saved token hist to {out_path2}\")\n",
    "print(articles_df[\"word_count\"].describe().round(1).to_string())\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Duplicate detection\n",
    "# Exact duplicates by headline\n",
    "dup_exact = articles_df[articles_df.duplicated(subset=[\"headline\"], keep=False)]\n",
    "print(f\"Exact headline duplicates: {len(dup_exact)} articles ({len(dup_exact) / max(len(articles_df), 1) * 100:.1f}%)\")\n",
    "\n",
    "# Cosine similarity sample (n=500 random pairs)\n",
    "SAMPLE_N = min(500, len(articles_df))\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    sample_rows = conn.execute(\n",
    "        text(\n",
    "            \"SELECT ae.id, ae.embedding FROM article_embeddings ae \"\n",
    "            \"ORDER BY RANDOM() LIMIT :n\"\n",
    "        ),\n",
    "        {\"n\": SAMPLE_N},\n",
    "    ).fetchall()\n",
    "\n",
    "if sample_rows:\n",
    "    vecs = []\n",
    "    for row in sample_rows:\n",
    "        emb = row[1]\n",
    "        vecs.append(json.loads(emb) if isinstance(emb, str) else list(emb))\n",
    "    vecs = np.array(vecs)\n",
    "\n",
    "    # Normalise\n",
    "    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1, norms)\n",
    "    vecs_norm = vecs / norms\n",
    "\n",
    "    # Sample 200 random pairs\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx_a = rng.integers(0, len(vecs_norm), 200)\n",
    "    idx_b = rng.integers(0, len(vecs_norm), 200)\n",
    "    cosines = np.sum(vecs_norm[idx_a] * vecs_norm[idx_b], axis=1)\n",
    "\n",
    "    near_dup_threshold = 0.95\n",
    "    near_dups = np.sum(cosines > near_dup_threshold)\n",
    "    print(f\"Near-duplicate pairs (cosine > {near_dup_threshold}) in sample: {near_dups} / 200\")\n",
    "    print(f\"Cosine similarity stats: mean={cosines.mean():.3f}, p95={np.percentile(cosines, 95):.3f}\")\n",
    "else:\n",
    "    print(\"No embeddings found in DB. Run embed step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — FSI correlation: article volume vs FSI over time (dual-axis chart)\n",
    "daily_counts = articles_df.groupby(\"date\").size().rename(\"article_count\").reset_index()\n",
    "merged = pd.merge(daily_counts, fsi_df, on=\"date\", how=\"inner\")\n",
    "\n",
    "if merged.empty:\n",
    "    print(\"No overlap between articles and FSI dates. Check data.\")\n",
    "else:\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.bar(merged[\"date\"], merged[\"article_count\"], color=\"steelblue\", alpha=0.6, label=\"Article count\")\n",
    "    ax2.plot(merged[\"date\"], merged[\"fsi_value\"], color=\"crimson\", linewidth=2, label=\"FSI\")\n",
    "\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Article count\", color=\"steelblue\")\n",
    "    ax2.set_ylabel(\"FSI value\", color=\"crimson\")\n",
    "    ax1.set_title(\"Daily article volume vs Financial Stress Index\")\n",
    "\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(handles1 + handles2, labels1 + labels2, loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path3 = docs_dir / \"eda_fsi_vs_volume.png\"\n",
    "    fig.savefig(out_path3, dpi=100)\n",
    "    print(f\"Saved FSI chart to {out_path3}\")\n",
    "\n",
    "    corr = merged[\"article_count\"].corr(merged[\"fsi_value\"])\n",
    "    print(f\"Pearson correlation article_count vs fsi_value: {corr:.3f}\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Summary table with quality gates\n",
    "gates = [\n",
    "    (\"Total articles\",         len(articles_df),  100,    \"articles in DB\"),\n",
    "    (\"Embeddings coverage\",    emb_count,         len(articles_df) * 0.95,  \"embeddings >= 95% of articles\"),\n",
    "    (\"FSI rows\",               len(fsi_df),       50,     \"FSI business days\"),\n",
    "    (\"Exact dup rate (%)\",     round(len(dup_exact) / max(len(articles_df), 1) * 100, 1), None, \"informational\"),\n",
    "]\n",
    "\n",
    "print(\"\\n=== Quality Gate Summary ===\")\n",
    "print(f\"{'Metric':<30} {'Value':>10} {'Threshold':>12} {'Pass?':>6}\")\n",
    "print(\"-\" * 62)\n",
    "all_pass = True\n",
    "for name, value, threshold, note in gates:\n",
    "    if threshold is None:\n",
    "        status = \"INFO\"\n",
    "    elif value >= threshold:\n",
    "        status = \"PASS\"\n",
    "    else:\n",
    "        status = \"FAIL\"\n",
    "        all_pass = False\n",
    "    thresh_str = str(threshold) if threshold is not None else \"N/A\"\n",
    "    print(f\"{name:<30} {value:>10} {thresh_str:>12} {status:>6}  # {note}\")\n",
    "\n",
    "print()\n",
    "if all_pass:\n",
    "    print(\"All required quality gates PASSED.\")\n",
    "else:\n",
    "    print(\"WARNING: Some quality gates FAILED. Check data ingestion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
